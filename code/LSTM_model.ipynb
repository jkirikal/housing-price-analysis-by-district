{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946f8476",
   "metadata": {},
   "source": [
    "# Housing price one-shot forecasting using a multivariate LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353d0ee",
   "metadata": {},
   "source": [
    "We want to predict housing prices in the future using the time series data currently available to us. Initially, we attempted to use a simple ARIMA model to forecast future prices, but we realized that such a basic model would not be sufficient for predicting values in an uncertain market like the housing market. Now, we are planning to implement a multivariate Long Short-Term Memory (LSTM) model for future forecasting. The LSTM network is a type of recurrent neural network (RNN) that can be employed to predict values based on time series data. One-shot forecasting means making a prediction for the following month and comparing it to the actual outcome before making the next prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48c0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71497b",
   "metadata": {},
   "source": [
    "## 1. Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cd23c",
   "metadata": {},
   "source": [
    "In addition to monthly average housing prices that we scraped, we also have the monthly number of housing advertisements on <a href=\"https://www.kv.ee/\">kv.ee</a>, housing loan interest rates over the years, and the average salary over the years.\n",
    "\n",
    "Firstly, we are going to read and refactor all the data from different sources and merge them into a single time series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fe0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(district=\"Tallinn\"):\n",
    "    # 1. Read and prepare housing data\n",
    "    data = pd.read_csv(\"../data/lstm/housingData.csv\",encoding='latin-1')\n",
    "    data.date = data.date.astype(\"str\")\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], format='%m-%Y')\n",
    "    data = data.replace('-', np.nan)\n",
    "    data = data.ffill() #Forward Fill \n",
    "    data.price = data.price.astype(\"float\")\n",
    "    data.advertisements = data.advertisements.astype(\"int\")\n",
    "    data = data.drop([\"actives\", \"city\"], axis=1)\n",
    "    data = data[data.district == district]\n",
    "    \n",
    "    # 2. Read and prepare housing loan interest rate data\n",
    "    dataInterestRate = pd.read_csv(\"../data/lstm/interestRateData.csv\", delimiter=';',  encoding=\"latin-1\")\n",
    "    dataInterestRate = dataInterestRate.drop(dataInterestRate.columns[0], axis=1)\n",
    "    dataInterestRate = dataInterestRate.drop(',,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', axis=1)\n",
    "    dataInterestRate = dataInterestRate.loc[[4]]\n",
    "    df_melted = pd.melt(dataInterestRate, var_name='date', value_name='interest_rate', ignore_index=False)\n",
    "    df_melted['interest_rate'] = df_melted['interest_rate'].str.replace(',', '.').str.replace('%', '').astype(float)\n",
    "    df_melted.reset_index(inplace=True)\n",
    "    dataInterestRate = dataInterestRate.drop(dataInterestRate.columns[0], axis=1)\n",
    "    df_melted[\"date\"] = pd.to_datetime(df_melted[\"date\"], format='%m.%Y')\n",
    "    df_melted = df_melted.drop(\"index\", axis=1)\n",
    "    \n",
    "    # 3. Read and prepare income data from first file\n",
    "    incomeData1 = pd.read_csv(\"../data/lstm/kuupalk2007-2018.csv\", skiprows=2,encoding='latin-1')\n",
    "    incomeData1['Maakond'] = incomeData1['Maakond'].replace({'..Tallinn': 'Tallinn', 'Tartu maakond': 'Tartu'})\n",
    "    incomeData1.rename(columns={'Keskmine brutokuupalk, eurot': 'salary'}, inplace=True)\n",
    "    incomeData1.rename(columns={'Maakond': 'district'}, inplace=True)\n",
    "    incomeData1.rename(columns={'Aasta': 'year'}, inplace=True)\n",
    "    incomeData1.rename(columns={'Keskmise brutokuupalga juurdekasvutempo vÃµrreldes eelmise perioodiga, %': 'difference'}, inplace=True)\n",
    "    incomeData1 = incomeData1[incomeData1.district == district]\n",
    "    #incomeData1['difference'] = incomeData1['difference'].astype(float)\n",
    "\n",
    "    kvartal_mapping = {'I kvartal': 1, 'II kvartal': 2, 'III kvartal': 3, 'IV kvartal': 4}\n",
    "    incomeData1['Kvartal'] = incomeData1['Kvartal'].map(kvartal_mapping)\n",
    "    incomeData1['year'] = incomeData1['year'].astype(int)\n",
    "\n",
    "    # Create a new DataFrame with 12 months for each year\n",
    "    expanded_dates = pd.DataFrame({\n",
    "        'year': np.repeat(incomeData1['year'].unique(), 12),\n",
    "        'month': np.tile(np.arange(1, 13), len(incomeData1['year'].unique()))\n",
    "    })\n",
    "\n",
    "    # Map each month to its corresponding quarter\n",
    "    month_to_quarter = {1: 1, 2: 1, 3: 1, 4: 2, 5: 2, 6: 2, 7: 3, 8: 3, 9: 3, 10: 4, 11: 4, 12: 4}\n",
    "    expanded_dates['Kvartal'] = expanded_dates['month'].map(month_to_quarter)\n",
    "\n",
    "    # Merge the expanded_dates DataFrame with the original DataFrame\n",
    "    result_df = pd.merge(expanded_dates, incomeData1, on=['year', 'Kvartal'], how='left')\n",
    "\n",
    "    # Forward fill the values for 'district', 'salary', and 'difference'\n",
    "    result_df[['district', 'salary', 'difference']] = result_df[['district', 'salary', 'difference']].ffill()\n",
    "    result_df['date'] = pd.to_datetime(result_df[['year', 'month']].assign(day=1))\n",
    "    result_df = result_df.drop([\"Kvartal\", \"year\", \"month\"], axis=1)\n",
    "    result_df[\"date\"] = pd.to_datetime(result_df[\"date\"], format='%Y-%M')\n",
    "\n",
    "    # 4. Read and prepare income data from second file\n",
    "    incomeData2 = pd.read_csv(\"../data/lstm/kuupalk2018-2022.csv\", skiprows=2,encoding='latin-1')\n",
    "    incomeData2 = incomeData2.drop(index=range(4)) # Starts from 2018, already have these values\n",
    "    incomeData2['Maakond'] = incomeData2['Maakond'].replace({'Tartu maakond': 'Tartu'})\n",
    "    incomeData2.rename(columns={'Maakond': 'district','Keskmise brutokuupalga muutus vÃµrreldes eelmise perioodiga, %': 'difference', \"Keskmine brutokuupalk, eurot\":\"salary\"}, inplace=True)\n",
    "    incomeData2.rename(columns={'Maakond': 'district'}, inplace=True)\n",
    "    incomeData2['difference'] = incomeData2['difference'].replace(\"..\", 6.6)\n",
    "    incomeData2[['year', 'quartile']] = incomeData2['Vaatlusperiood'].str.split(' ', n=1, expand=True)\n",
    "    quartile_to_int = {'I kvartal': 1, 'II kvartal': 2, 'III kvartal': 3, 'IV kvartal': 4}\n",
    "    incomeData2['quartile'] = incomeData2['quartile'].map(quartile_to_int)\n",
    "    incomeData2['year'] = incomeData2['year'].astype(int)\n",
    "    incomeData2 = incomeData2.drop(\"Vaatlusperiood\", axis=1)\n",
    "    incomeData2['difference'] = incomeData2['difference'].astype(float)\n",
    "    incomeData2 = incomeData2[incomeData2['district'] == district]\n",
    "\n",
    "    # Create a new DataFrame with 12 months for each year\n",
    "    expanded_dates = pd.DataFrame({\n",
    "        'year': np.repeat(incomeData2['year'].unique(), 12),\n",
    "        'month': np.tile(np.arange(1, 13), len(incomeData2['year'].unique()))\n",
    "    })\n",
    "\n",
    "    expanded_dates['quartile'] = expanded_dates['month'].map(month_to_quarter)\n",
    "\n",
    "    # Merge the expanded_dates DataFrame with the original DataFrame\n",
    "    result_df2 = pd.merge(expanded_dates, incomeData2, on=['year', 'quartile'], how='left')\n",
    "\n",
    "    # Forward fill the values for 'district', 'salary', and 'difference'\n",
    "    result_df2[['district', 'salary', 'difference']] = result_df2[['district', 'salary', 'difference']].ffill()\n",
    "    result_df2['date'] = pd.to_datetime(result_df2[['year', 'month']].assign(day=1))\n",
    "    result_df2 = result_df2.drop([\"year\", \"month\", \"quartile\"], axis=1)\n",
    "    result_df2[\"date\"] = pd.to_datetime(result_df2[\"date\"], format='%Y-%M')\n",
    "\n",
    "    # Merge both salary dataframes together\n",
    "    data_salary = pd.merge(result_df, result_df2, on=None, how='outer')\n",
    "    data_salary = data_salary.sort_values('date')  # Sort the merged DataFrame by the 'date' column\n",
    "    data_salary = data_salary.drop([\"district\"], axis=1)\n",
    "    \n",
    "    # 5. Merge the datasets\n",
    "    df_merged = pd.merge(data, df_melted, on='date', how='left')\n",
    "    df_merged = pd.merge(df_merged, data_salary, on='date', how='left')\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf40aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>district</th>\n",
       "      <th>price</th>\n",
       "      <th>advertisements</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>salary</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1993.2</td>\n",
       "      <td>7252</td>\n",
       "      <td>5.63</td>\n",
       "      <td>840.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1948.8</td>\n",
       "      <td>7405</td>\n",
       "      <td>5.65</td>\n",
       "      <td>840.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1935.0</td>\n",
       "      <td>7644</td>\n",
       "      <td>5.87</td>\n",
       "      <td>799.0</td>\n",
       "      <td>-4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1922.1</td>\n",
       "      <td>7945</td>\n",
       "      <td>5.98</td>\n",
       "      <td>799.0</td>\n",
       "      <td>-4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-09-01</td>\n",
       "      <td>Tallinn</td>\n",
       "      <td>1912.4</td>\n",
       "      <td>8476</td>\n",
       "      <td>6.09</td>\n",
       "      <td>799.0</td>\n",
       "      <td>-4.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date district   price  advertisements  interest_rate  salary  \\\n",
       "0 2007-05-01  Tallinn  1993.2            7252           5.63   840.0   \n",
       "1 2007-06-01  Tallinn  1948.8            7405           5.65   840.0   \n",
       "2 2007-07-01  Tallinn  1935.0            7644           5.87   799.0   \n",
       "3 2007-08-01  Tallinn  1922.1            7945           5.98   799.0   \n",
       "4 2007-09-01  Tallinn  1912.4            8476           6.09   799.0   \n",
       "\n",
       "   difference  \n",
       "0         8.0  \n",
       "1         8.0  \n",
       "2        -4.9  \n",
       "3        -4.9  \n",
       "4        -4.9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = readData(\"Tallinn\")\n",
    "data = data.dropna(subset=[\"salary\"]) # Since one time series dataset ended a year earlier, we have to remove those values\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7f4fd",
   "metadata": {},
   "source": [
    "## 2. Refactoring data and feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a6a6f",
   "metadata": {},
   "source": [
    "Next, we are going to refactor the data to a shape that the LSTM needs for input. Firstly, we are normalizing the dataset using min-max scaling. We are doing so because neural networks, including LSTMs, often converge faster when the input features are on a similar scale. Then, we are going to split the data into a training and test set. Each row will have the price as a label, and the values of the previous 60 rows (5 years) will serve as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42717397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler_price = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_advertisements = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_interest_rate = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_price = scaler_price.fit_transform(data[['price']])\n",
    "scaled_advertisements = scaler_advertisements.fit_transform(data[['advertisements']])\n",
    "scaled_interest_rate = scaler_interest_rate.fit_transform(data[['interest_rate']])\n",
    "\n",
    "scaled_data = np.concatenate((scaled_price, scaled_advertisements, scaled_interest_rate, data[['difference']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set\n",
    "train_length = int(len(scaled_data) * 0.8)\n",
    "val_length = int(len(scaled_data) * 0.1)\n",
    "\n",
    "train_data = scaled_data[0:train_length,:]\n",
    "val_data = scaled_data[train_length-60:train_length+val_length,:]\n",
    "test_data = scaled_data[train_length+val_length-60:,:]\n",
    "\n",
    "\n",
    "# 1. Create the training data set\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i,:]) # For every row take the values for past 60 rows\n",
    "    y_train.append(train_data[i,0]) # Actual price \n",
    "    \n",
    "# Convert the x_train and y_train to numpy arrays and reshaping\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 4))\n",
    "\n",
    "\n",
    "\n",
    "# 2. Create the validation data set\n",
    "x_val = []\n",
    "y_val = data.iloc[train_length:train_length+val_length, :]['price']\n",
    "\n",
    "for i in range(60, len(val_data)):\n",
    "    x_val.append(val_data[i-60:i, :])\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "x_val = np.array(x_val)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Create the testing data set\n",
    "x_test = []\n",
    "y_test = data.iloc[train_length+val_length:, :]['price']\n",
    "\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i-60:i, :])\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "x_test = np.array(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d90a4",
   "metadata": {},
   "source": [
    "## 3. Defining model arcitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df30175",
   "metadata": {},
   "source": [
    "We are going to specify the model architecture that will be used going forward. The architecture includes an LSTM layer, followed by densely connected layers. The model is trained using mean squared error (mse) loss and the Adam optimizer. All other values, such as the node count on each layer and data used for model fitting, can be passed into the function as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdba280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(x_train, y_train, config): \n",
    "    layer1_nodes,layer2_nodes,layer3_nodes, n_batch, n_epochs = config\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(x_train.shape[1], 4)))\n",
    "    model.add(LSTM(layer1_nodes, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(layer2_nodes))\n",
    "    model.add(Dense(layer3_nodes))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50fade8",
   "metadata": {},
   "source": [
    "## 4. Using grid search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141cbb4b",
   "metadata": {},
   "source": [
    "To determine the optimal hyperparameters for our model, we opted to employ grid search. Initially, our intention was to use <code>GridSearchCV</code> from the scikit-learn library. However, after encountering numerous errors using scikeras wrappers for the Tensorflow model and finding limited helpful information online, we decided to resort to manual implementation.\n",
    "\n",
    "**The function callout for grid search is commented out** since its execution time is prolonged. The first grid search we conducted lasted for 18 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_configs(): # Define the hyperparameter grid\n",
    " layer1_nodes = [50, 75, 100]\n",
    " layer2_nodes = [25, 50, 75, 100]\n",
    " layer3_nodes = [25, 50, 75]\n",
    " n_batch = [32, 64]\n",
    " n_epochs = [400, 700, 1000, 1400]\n",
    " \n",
    " configs = list()\n",
    " for i in layer1_nodes:\n",
    "    for j in layer2_nodes:\n",
    "         for k in layer3_nodes:\n",
    "            for m in n_batch:\n",
    "                for o in n_epochs:\n",
    "                    configs.append([i, j, k, m, o])\n",
    " print('Total configs: %d' % len(configs))\n",
    " return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_train, y_train, x_val, y_val, config):\n",
    "    model = create_lstm_model(x_train, y_train, config)\n",
    "    predictions = model.predict(x_val)\n",
    "    predictions = scaler_price.inverse_transform(predictions) # Undo scaling\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55427da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_evaluate(x_train, y_train, x_val, y_val, config, n_repeats=15):\n",
    "    evaluation_scores = []\n",
    "    \n",
    "    # fit and evaluate the model n times\n",
    "    for _ in range(n_repeats):\n",
    "        validation_score = evaluate(x_train, y_train, x_val, y_val, config)\n",
    "        evaluation_scores.append(validation_score)\n",
    "        \n",
    "    key = str(config)\n",
    "    print(\"Evaluation scores: \", evaluation_scores)\n",
    "    result = mean(evaluation_scores)\n",
    "    print('> Model[%s] received RMSE %.3f' % (key, result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(x_train, y_train, x_val, y_val, configurations):\n",
    "    \n",
    "    # Evaluate configurations and store results in a list\n",
    "    evaluated_configs = []\n",
    "    for config in configurations:\n",
    "        evaluation_result = repeat_evaluate(x_train, y_train, x_val, y_val, config, 20)\n",
    "        evaluated_configs.append((config, evaluation_result)) # store the configuration and repeated evaluation score as a pair \n",
    "\n",
    "    # Sort configurations by error in ascending order\n",
    "    sorted_configs = sorted(evaluated_configs, key=lambda x: x[1])\n",
    "    return sorted_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aed43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = model_configs()\n",
    "scores = []\n",
    "\n",
    "#%time scores = grid_search(x_train, y_train,x_val, y_val, configs)\n",
    "\n",
    "# Find the three configurations with the best error values\n",
    "for configuration, error in scores[:3]:\n",
    " print(configuration, error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9ee74",
   "metadata": {},
   "source": [
    "## 5. Building the final model and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756457f2",
   "metadata": {},
   "source": [
    "We are going to fit the final LSTM model with hyperparameters we found using grid search in the previous step. For fitting we are going to use all the available training + validation data. After the model has been trained, we will predict values for the testing data and calculate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ed6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(x_train.shape[1], 4)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(75))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=1000)\n",
    "\n",
    "# Get the models predicted price values.\n",
    "# We are combining and predicting values for both the validation and test datasets for better visualization on the plot\n",
    "\n",
    "# Concatenate x_val and x_test\n",
    "x_combined = np.concatenate((x_val, x_test), axis=0)\n",
    "predictions = model.predict(x_combined)\n",
    "predictions = scaler_price.inverse_transform(predictions) # Undo scaling\n",
    "\n",
    "# Calculate RMSE\n",
    "y_combined = np.concatenate((y_val, y_test), axis=0)\n",
    "rmse = np.sqrt(mean_squared_error(y_combined, predictions))\n",
    "print('Root Mean Square Error:', rmse, \" €\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e4eea",
   "metadata": {},
   "source": [
    "## 6. Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5dccf",
   "metadata": {},
   "source": [
    "Lastly, we are plotting the one-shot forecasting predictions of the model together with the actual values to gain a visual understanding of how well the model is performing. To scale the plot appropriately and display predictions on a larger scale, we are only plotting the training data starting from the year 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19800ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[72:train_length]\n",
    "test = data[train_length:].copy()\n",
    "test.loc[72:, 'Predictions'] = predictions\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.ylabel('Average Housing Price')\n",
    "plt.plot(train[\"date\"], train['price'])\n",
    "plt.plot(test[\"date\"], test[['price', 'Predictions']])\n",
    "plt.legend(['Training', 'Test', 'Predictions'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
